# hw2_report.ipynb

## Task A: Performance Comparison between BCE Loss and CE Loss

### Objective
Compare and analyze the performance of the model during training and testing phases when using BCE loss and CE loss in terms of loss and accuracy.

### Analysis
Binary Cross-Entropy (BCE) loss is specifically designed for binary classification tasks. It measures the difference between two probability distributions: the true distribution and the predicted distribution. On the other hand, Cross-Entropy (CE) loss is more general and can be used for multi-class classification tasks.

#### Training Phase
- **With BCE Loss**: The model might converge faster for binary classification tasks as it is specifically tailored for such tasks. The loss values might be lower compared to CE loss.
- **With CE Loss**: The model might take slightly longer to converge for binary classification tasks. However, it's more versatile and can be extended to multi-class classification tasks.

#### Testing Phase
- **With BCE Loss**: The accuracy might be slightly higher for binary classification tasks as the model is optimized using a loss function specifically designed for such tasks.
- **With CE Loss**: The accuracy might be slightly lower for binary classification tasks. However, the model is more robust and can generalize better to unseen data.

## Task B: Performance Comparison of Different Hyperparameters

### Author names should appear as used for conventional publication, with first and middle names or initials followed by surname. Every effort should be made to keep author names consistent from one paper to the next as they appear within our publications.

### Objective
Choose two hyperparameters and select three different values for each hyperparameter for experimentation. Train and test using the provided chest X-ray dataset.

### Hyperparameters
**Hyperparameter 1: Learning Rate**
- Values: 0.001, 0.01, 0.1

**Hyperparameter 2: Batch Size**
- Values: 32, 64, 128

### Analysis

#### Learning Rate
- **0.001**: The model might converge slowly but can achieve a better local minimum.
- **0.01**: A balanced learning rate that provides a trade-off between convergence speed and accuracy.
- **0.1**: The model might converge faster but can overshoot the optimal point.

#### Batch Size
- **32**: Provides a good balance between memory usage and convergence speed. Might achieve a better local minimum.
- **64**: Uses more memory but can converge faster than a batch size of 32.
- **128**: Might converge the fastest among the three but requires the most memory.

